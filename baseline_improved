{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data + Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
      "0  11_0       11         0          0  1404575400000   \n",
      "1  11_0       11         0          0  1404575400000   \n",
      "2  11_0       11         0          0  1404575400000   \n",
      "3  11_0       11         0          0  1404575400000   \n",
      "4  11_0       11         0          0  1404575400000   \n",
      "\n",
      "                                               Tweet  \n",
      "0     time to focus on belgium winning the world cup  \n",
      "1  i just hope argentina lose would be fun to see...  \n",
      "2  watch argentinabelgium 5th july 2014 live go t...  \n",
      "3                        why dont you like argentina  \n",
      "4  even though i hate belgium for beating the us ...  \n"
     ]
    }
   ],
   "source": [
    "# Read all training files (preprocessed) and concatenate them into one dataframe\n",
    "li = []\n",
    "for filename in os.listdir(\"train_tweets_preprocessed_soft\"):\n",
    "    df = pd.read_csv(\"train_tweets_preprocessed_soft/\" + filename)\n",
    "    li.append(df)\n",
    "df = pd.concat(li, ignore_index=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 573189), ('rt', 406433), ('germany', 390846), ('to', 371354), ('i', 293908), ('a', 280479), ('is', 254944), ('for', 235092), ('in', 223224), ('and', 208825), ('brazil', 200749), ('this', 184137), ('of', 168489), ('on', 168342), ('it', 128723), ('that', 126470), ('france', 122804), ('you', 119651), ('win', 112514), ('game', 110999)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter the dataframe based on TestType == 1\n",
    "sub_event_df = df[df['EventType'] == 1]\n",
    "all_tweets = \" \".join(sub_event_df['Tweet'].astype(str))\n",
    "\n",
    "# Perform word frequency analysis\n",
    "words = re.findall(r'\\w+', all_tweets.lower())\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# Get the most common words\n",
    "most_common = word_freq.most_common(20)\n",
    "top_words, top_counts = zip(*most_common)\n",
    "\n",
    "# Print the most common words\n",
    "print(most_common)\n",
    "\n",
    "# Plot the most common words\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(top_words, top_counts)\n",
    "plt.title(\"Most common words EventType = 1\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('win', 131324), ('go', 101788), ('game', 92323), ('goal', 70738), ('like', 65527), ('cup', 65079), ('let', 58817), ('come', 57268), ('team', 55283), ('score', 55034), ('not', 54604), ('get', 42800), ('need', 41421), ('want', 40800), ('dont', 40498), ('time', 40174), ('playing', 40145), ('one', 39797), ('good', 39437), ('match', 35665)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter the dataframe based on TestType == 0\n",
    "non_sub_event_df = df[df['EventType'] == 0]\n",
    "all_tweets = \" \".join(non_sub_event_df['Tweet'].astype(str))\n",
    "\n",
    "# Perform word frequency analysis\n",
    "words = re.findall(r'\\w+', all_tweets.lower())\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# Get the most common words\n",
    "most_common = word_freq.most_common(20)\n",
    "top_words, top_counts = zip(*most_common)\n",
    "\n",
    "# Print the most common words\n",
    "print(most_common)\n",
    "\n",
    "# Plot the most common words\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(top_words, top_counts)\n",
    "plt.title(\"Most common words EventType = 1\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m text_dis  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sub_event_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m))\n\u001b[1;32m      2\u001b[0m text_ndis \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(non_sub_event_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m wordcloud_dis  \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_dis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m wordcloud_ndis \u001b[38;5;241m=\u001b[39m WordCloud()\u001b[38;5;241m.\u001b[39mgenerate(text_ndis)\n\u001b[1;32m      7\u001b[0m fig, (ax1,ax2) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/wordcloud/wordcloud.py:642\u001b[0m, in \u001b[0;36mWordCloud.generate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m    628\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/wordcloud/wordcloud.py:623\u001b[0m, in \u001b[0;36mWordCloud.generate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m    607\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 623\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_from_frequencies(words)\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/wordcloud/wordcloud.py:598\u001b[0m, in \u001b[0;36mWordCloud.process_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    596\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([i\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopwords])\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollocations:\n\u001b[0;32m--> 598\u001b[0m     word_counts \u001b[38;5;241m=\u001b[39m \u001b[43munigrams_and_bigrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_plurals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollocation_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;66;03m# remove stopwords\u001b[39;00m\n\u001b[1;32m    601\u001b[0m     words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/wordcloud/tokenization.py:43\u001b[0m, in \u001b[0;36munigrams_and_bigrams\u001b[0;34m(words, stopwords, normalize_plurals, collocation_threshold)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munigrams_and_bigrams\u001b[39m(words, stopwords, normalize_plurals\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collocation_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# We must create the bigrams before removing the stopword tokens from the words, or else we get bigrams like\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# \"thank much\" from \"thank you very much\".\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# We don't allow any of the words in the bigram to be stopwords\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     bigrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpairwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     unigrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords)\n\u001b[1;32m     45\u001b[0m     n_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(unigrams)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/wordcloud/tokenization.py:43\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munigrams_and_bigrams\u001b[39m(words, stopwords, normalize_plurals\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collocation_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# We must create the bigrams before removing the stopword tokens from the words, or else we get bigrams like\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# \"thank much\" from \"thank you very much\".\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# We don't allow any of the words in the bigram to be stopwords\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     bigrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pairwise(words) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m stopwords \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m p))\n\u001b[1;32m     44\u001b[0m     unigrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords)\n\u001b[1;32m     45\u001b[0m     n_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(unigrams)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text_dis  = \" \".join(sub_event_df['Tweet'].astype(str))\n",
    "text_ndis = \" \".join(non_sub_event_df['Tweet'].astype(str))\n",
    "\n",
    "wordcloud_dis  = WordCloud().generate(text_dis)\n",
    "wordcloud_ndis = WordCloud().generate(text_ndis)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2,figsize=(12,6))\n",
    "\n",
    "ax1.imshow(wordcloud_dis)\n",
    "ax1.axis(\"off\")\n",
    "ax1.set_title('Wordcloud for the sub event Tweets',fontsize=20)\n",
    "\n",
    "ax2.imshow(wordcloud_ndis)\n",
    "ax2.axis(\"off\")\n",
    "ax2.set_title('Wordcloud for the Non-sub event Tweets',fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'period_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# We split our data into a training and test set that we can use to train our classifier without fine-tuning into the\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# validation set and without submitting too many times into Kaggle\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# We drop the non-numerical features and keep the embeddings values for each period\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mperiod_features\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEventType\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatchID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPeriodID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# We extract the labels of our training samples\u001b[39;00m\n\u001b[1;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m period_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEventType\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[0;31mNameError\u001b[0m: name 'period_features' is not defined"
     ]
    }
   ],
   "source": [
    "# We split our data into a training and test set that we can use to train our classifier without fine-tuning into the\n",
    "# validation set and without submitting too many times into Kaggle\n",
    "\n",
    "# We drop the non-numerical features and keep the embeddings values for each period\n",
    "X = period_features.drop(columns=['EventType', 'MatchID', 'PeriodID', 'ID']).values\n",
    "\n",
    "# We extract the labels of our training samples\n",
    "y = period_features['EventType'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe model with Gensim's API\n",
    "embeddings_model = api.load(\"glove-twitter-200\")  # 200-dimensional GloVe embeddings\n",
    "\n",
    "# Save the model in a binary format for quick reloading\n",
    "embeddings_model.save(\"glove-twitter-200.kv\")\n",
    "\n",
    "embeddings_model = KeyedVectors.load(\"glove-twitter-200.kv\") #fast but does not have any contextual information #leads to a test accuracy of 75%\n",
    "#embeddings_model = KeyedVectors.load_word2vec_format(\"crawl-300d-2M.vec\", binary=False) #yields 74% of test accuracy\n",
    "#Fast-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the average word vector for a tweet\n",
    "def get_avg_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    print(words)\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "def get_sum_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.sum(word_vectors, axis=0)\n",
    "\n",
    "def get_min_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.min(word_vectors, axis=0)\n",
    "\n",
    "def get_max_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.max(word_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and its embeddings\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to each tweet and obtain vectors\n",
    "vector_size = 200  # Adjust based on the chosen GloVe model\n",
    "#tweet_vectors = np.vstack([get_avg_embedding(str(tweet), embeddings_model, vector_size) for tweet in df['Tweet']]) #77% in logisitc regression\n",
    "#tweet_vectors = np.vstack([get_sum_embedding(str(tweet), embeddings_model, vector_size) for tweet in df['Tweet']]) #75% in logistic regression\n",
    "tweet_vectors = np.vstack([get_min_embedding(str(tweet), embeddings_model, vector_size) for tweet in df['Tweet']])  #79% in logistic regression\n",
    "#tweet_vectors = np.vstack([get_max_embedding(str(tweet), embeddings_model, vector_size) for tweet in df['Tweet']])  #77% in logistic regression\n",
    "\n",
    "tweet_df = pd.DataFrame(tweet_vectors)\n",
    "\n",
    "print(\"and its embeddings\")\n",
    "\n",
    "# Attach the vectors into the original dataframe\n",
    "period_features = pd.concat([df, tweet_df], axis=1)\n",
    "\n",
    "# Drop the columns that are not useful anymore\n",
    "period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
    "\n",
    "# Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "\n",
    "# We drop the non-numerical features and keep the embeddings values for each period\n",
    "X = period_features.drop(columns=['EventType', 'MatchID', 'PeriodID', 'ID']).values\n",
    "\n",
    "# We extract the labels of our training samples\n",
    "y = period_features['EventType'].values# Function to compute the average word vector for a tweet\n",
    "\n",
    "# We split our data into a training and test set that we can use to train our classifier without fine-tuning into the\n",
    "# validation set and without submitting too many times into Kaggle\n",
    "\n",
    "# We drop the non-numerical features and keep the embeddings values for each period\n",
    "X = period_features.drop(columns=['EventType', 'MatchID', 'PeriodID', 'ID']).values\n",
    "\n",
    "# We extract the labels of our training samples\n",
    "y = period_features['EventType'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load pretrained sentence transformer model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "# Encode sentences\n",
    "texts = [\"Example sentence\", \"Another example\"]\n",
    "embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "print(embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weve got the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 93300/93300 [10:15<00:00, 151.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and its embeddings\n"
     ]
    }
   ],
   "source": [
    "# Read all training files (preprocessed) and concatenate them into one dataframe\n",
    "\n",
    "li = []\n",
    "for filename in os.listdir(\"train_tweets_preprocessed\"):\n",
    "    df = pd.read_csv(\"train_tweets_preprocessed/\" + filename)\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li, ignore_index=True)\n",
    "print(\"weve got the dataset\")\n",
    "\n",
    "# Apply preprocessing to each tweet and obtain vectors\n",
    "\n",
    "tweet_vectors = model.encode(\n",
    "    df['Tweet'].astype(str).tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32,  # Adjust batch size based on available GPU memory\n",
    "    convert_to_tensor=True\n",
    ")\n",
    "tweet_vectors = tweet_vectors.cpu().numpy()  # Move to CPU if necessary\n",
    "\n",
    "tweet_df = pd.DataFrame(tweet_vectors)\n",
    "print(\"and its embeddings\")\n",
    "\n",
    "# Attach the vectors into the original dataframe\n",
    "period_features = pd.concat([df, tweet_df], axis=1)\n",
    "\n",
    "# Drop the columns that are not useful anymore\n",
    "period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
    "\n",
    "# Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "\n",
    "# We drop the non-numerical features and keep the embeddings values for each period\n",
    "X = period_features.drop(columns=['EventType', 'MatchID', 'PeriodID', 'ID']).values\n",
    "\n",
    "# We extract the labels of our training samples\n",
    "y = period_features['EventType'].values\n",
    "\n",
    "# We split our data into a training and test set that we can use to train our classifier without fine-tuning into the\n",
    "# validation set and without submitting too many times into Kaggle\n",
    "\n",
    "# We drop the non-numerical features and keep the embeddings values for each period\n",
    "X = period_features.drop(columns=['EventType', 'MatchID', 'PeriodID', 'ID']).values\n",
    "\n",
    "# We extract the labels of our training samples\n",
    "y = period_features['EventType'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing huggingface_hub.hf_api: No module named 'requests'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17297/3112523913.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-cased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# you may not use this file except in compliance with the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# You may obtain a copy of the License at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;31m#     http://www.apache.org/licenses/LICENSE-2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Unless required by applicable law or agreed to in writing, software\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# you may not use this file except in compliance with the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# You may obtain a copy of the License at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#!/usr/bin/env python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# coding=utf-8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;31m# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# you may not use this file except in compliance with the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/__init__.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0msubmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmod_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error importing {submod_path}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m             \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;31m# If the attribute lives in a file (module) with the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.9/importlib/__init__.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcharacter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcharacter\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m from typing import (\n\u001b[1;32m     44\u001b[0m     \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mBinaryIO\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mIterable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "train_encodings = tokenizer(X_train.to_list(), truncation=True, padding=True)\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 2\n",
    "# num_train_steps is the number of mini-batches the model is going to see over its entire training:\n",
    "mini_batches_per_epoch = int(len(X_train) / batch_size)\n",
    "num_train_steps =  mini_batches_per_epoch * num_epochs\n",
    "# here we are using warm-up then a constant/linear decay from initial_learning_rate to zero over num_train_steps:\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=3.6e-5,\n",
    "    num_warmup_steps=0.1*num_train_steps,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0,\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune Bert on the soft version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-12 10:48:47.570258: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-12 10:48:48.526680: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733996928.866207  163459 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733996928.963392  163459 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-12 10:48:49.867270: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model_name = \"bert-base-cased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Identifying the model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_name = \"google/electra-small-discriminator\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 2089904\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 895674\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2089904/2089904 [00:34<00:00, 61430.49 examples/s]\n",
      "Map: 100%|██████████| 895674/895674 [00:14<00:00, 60162.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "df['Tweet'] = df['Tweet'].apply(str)\n",
    "X = df['Tweet'].values\n",
    "y = df['EventType'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "df_train = pd.DataFrame(data = {\"text\": X_train, 'label' : y_train})\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "print(train_dataset)\n",
    "\n",
    "df_test = pd.DataFrame(data = {\"text\": X_test, 'label' : y_test})\n",
    "test_dataset = Dataset.from_pandas(df_test) \n",
    "print(test_dataset)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "   return tokenizer(examples[\"text\"], padding=\"max_length\", max_length= 20,truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "Trainer,    \n",
    "TrainingArguments\n",
    ")\n",
    "training_args = TrainingArguments(\n",
    "output_dir = \"./fine_tuned_model_electra\", \n",
    "num_train_epochs = 8,\n",
    "save_steps = 10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='159516' max='2089904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 159516/2089904 33:30 < 6:45:31, 79.34 it/s, Epoch 0.61/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.671800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.671700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.659200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.673100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.673500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.669400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.673500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.672700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.668400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.669200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.673400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.673700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.682800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.675900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.674100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.678300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.676700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.677300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.677500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.672700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.676100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.673400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.678400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.677100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.676000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.677300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.674300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.679200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.674600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.671400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.674000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.677600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.671600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.669500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.678300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.675600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.671100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.675400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.680700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.675300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.677300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.676000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.672700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.681300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.684100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.683100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.687400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.688200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.685300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.685600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.683300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.679600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.676700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.681300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.682900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.679500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.676500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.679700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.682200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.678800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.678300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.680600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.680500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.677100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.674300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.683300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.674900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.673900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.680700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.676600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.677000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.679900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.678600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.680700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.676700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.677600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.680700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.679800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.683000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.679600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.679200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>0.680200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.672900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>0.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>0.686300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.687800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>0.684900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>0.683400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>0.679900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.688100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>0.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.683100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>0.685300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.682300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>0.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>0.686800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.682600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>0.682800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.685100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>0.681900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>0.683500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>0.687600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.682300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>0.682800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>0.680800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.687100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>0.683600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.686300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>0.684600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.682700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>0.682800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>0.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.686500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>0.685100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.683200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>0.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>0.684700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.687800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>0.687300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.683400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75500</td>\n",
       "      <td>0.685600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>0.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>0.684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>0.685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.681900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>0.684700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>0.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79500</td>\n",
       "      <td>0.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.686500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80500</td>\n",
       "      <td>0.682500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>0.686800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81500</td>\n",
       "      <td>0.684900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.684100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82500</td>\n",
       "      <td>0.687100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>0.681800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83500</td>\n",
       "      <td>0.685600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.684200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84500</td>\n",
       "      <td>0.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85500</td>\n",
       "      <td>0.683500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86500</td>\n",
       "      <td>0.685300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>0.685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87500</td>\n",
       "      <td>0.686200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.686100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88500</td>\n",
       "      <td>0.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>0.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89500</td>\n",
       "      <td>0.685600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90500</td>\n",
       "      <td>0.686300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>0.684500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91500</td>\n",
       "      <td>0.688200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.682700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92500</td>\n",
       "      <td>0.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>0.687400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93500</td>\n",
       "      <td>0.684100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.687200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94500</td>\n",
       "      <td>0.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>0.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95500</td>\n",
       "      <td>0.682300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.687300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96500</td>\n",
       "      <td>0.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97000</td>\n",
       "      <td>0.684600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97500</td>\n",
       "      <td>0.682500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98500</td>\n",
       "      <td>0.684700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>0.681500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99500</td>\n",
       "      <td>0.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.687100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100500</td>\n",
       "      <td>0.685300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101000</td>\n",
       "      <td>0.685300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101500</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.683000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102500</td>\n",
       "      <td>0.684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103000</td>\n",
       "      <td>0.688300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103500</td>\n",
       "      <td>0.685600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.681100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104500</td>\n",
       "      <td>0.684300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>0.686300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105500</td>\n",
       "      <td>0.683100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106500</td>\n",
       "      <td>0.689200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107000</td>\n",
       "      <td>0.682900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107500</td>\n",
       "      <td>0.686200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108500</td>\n",
       "      <td>0.687300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109000</td>\n",
       "      <td>0.685300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109500</td>\n",
       "      <td>0.684900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.682100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110500</td>\n",
       "      <td>0.683700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111000</td>\n",
       "      <td>0.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111500</td>\n",
       "      <td>0.684200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112500</td>\n",
       "      <td>0.685700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113000</td>\n",
       "      <td>0.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113500</td>\n",
       "      <td>0.685300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114500</td>\n",
       "      <td>0.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>0.683200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115500</td>\n",
       "      <td>0.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.683700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116500</td>\n",
       "      <td>0.686800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117000</td>\n",
       "      <td>0.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117500</td>\n",
       "      <td>0.683300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118500</td>\n",
       "      <td>0.677100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119000</td>\n",
       "      <td>0.686300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119500</td>\n",
       "      <td>0.683100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.687600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120500</td>\n",
       "      <td>0.680600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121000</td>\n",
       "      <td>0.685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121500</td>\n",
       "      <td>0.684100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122500</td>\n",
       "      <td>0.682300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123000</td>\n",
       "      <td>0.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123500</td>\n",
       "      <td>0.685700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.687900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124500</td>\n",
       "      <td>0.684100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>0.684300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125500</td>\n",
       "      <td>0.684100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.684700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126500</td>\n",
       "      <td>0.685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127000</td>\n",
       "      <td>0.683300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127500</td>\n",
       "      <td>0.689100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128500</td>\n",
       "      <td>0.684300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129000</td>\n",
       "      <td>0.687100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129500</td>\n",
       "      <td>0.686800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.684100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130500</td>\n",
       "      <td>0.685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131000</td>\n",
       "      <td>0.681500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131500</td>\n",
       "      <td>0.681700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>0.683500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132500</td>\n",
       "      <td>0.685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133000</td>\n",
       "      <td>0.687400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133500</td>\n",
       "      <td>0.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>0.684900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134500</td>\n",
       "      <td>0.684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>0.684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135500</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>0.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136500</td>\n",
       "      <td>0.687600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137000</td>\n",
       "      <td>0.684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137500</td>\n",
       "      <td>0.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>0.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138500</td>\n",
       "      <td>0.682200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139000</td>\n",
       "      <td>0.684100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139500</td>\n",
       "      <td>0.684900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.687600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140500</td>\n",
       "      <td>0.684200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141000</td>\n",
       "      <td>0.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141500</td>\n",
       "      <td>0.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142000</td>\n",
       "      <td>0.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142500</td>\n",
       "      <td>0.683700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143000</td>\n",
       "      <td>0.684500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143500</td>\n",
       "      <td>0.688700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>0.686100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144500</td>\n",
       "      <td>0.688200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145000</td>\n",
       "      <td>0.686500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145500</td>\n",
       "      <td>0.684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146000</td>\n",
       "      <td>0.683400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146500</td>\n",
       "      <td>0.686200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147000</td>\n",
       "      <td>0.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147500</td>\n",
       "      <td>0.682400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148000</td>\n",
       "      <td>0.684700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148500</td>\n",
       "      <td>0.685100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149000</td>\n",
       "      <td>0.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149500</td>\n",
       "      <td>0.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>0.686200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150500</td>\n",
       "      <td>0.686900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151000</td>\n",
       "      <td>0.688900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151500</td>\n",
       "      <td>0.686900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152000</td>\n",
       "      <td>0.684600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152500</td>\n",
       "      <td>0.684200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153000</td>\n",
       "      <td>0.687800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153500</td>\n",
       "      <td>0.682400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154000</td>\n",
       "      <td>0.684900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154500</td>\n",
       "      <td>0.689300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155000</td>\n",
       "      <td>0.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155500</td>\n",
       "      <td>0.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156000</td>\n",
       "      <td>0.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156500</td>\n",
       "      <td>0.686100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157000</td>\n",
       "      <td>0.685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157500</td>\n",
       "      <td>0.682300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158000</td>\n",
       "      <td>0.683300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158500</td>\n",
       "      <td>0.683600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159000</td>\n",
       "      <td>0.683500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159500</td>\n",
       "      <td>0.685500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure the model is using GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for '/users/eleves-a/2022/oussama.zouhry/sub-event-detection-in-twitter-streams/challenge_data/fine_tuned_model_bert_cased/checkpoint-12000/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/users/eleves-a/2022/oussama.zouhry/sub-event-detection-in-twitter-streams/challenge_data/fine_tuned_model_bert_cased/checkpoint-12000/' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#trainer.save_model(local_path)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:940\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    937\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 940\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    942\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2016\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2013\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2014\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2019\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '/users/eleves-a/2022/oussama.zouhry/sub-event-detection-in-twitter-streams/challenge_data/fine_tuned_model_bert_cased/checkpoint-12000/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/users/eleves-a/2022/oussama.zouhry/sub-event-detection-in-twitter-streams/challenge_data/fine_tuned_model_bert_cased/checkpoint-12000/' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "local_path = \"/users/eleves-a/2022/oussama.zouhry/sub-event-detection-in-twitter-streams/challenge_data/fine_tuned_model_bert_cased/checkpoint-12000\"\n",
    "\n",
    "local_path = \"/users/eleves-a/2022/oussama.zouhry/sub-event-detection-in-twitter-streams/challenge_data/fine_tuned_model_bert_cased/checkpoint-12000/\"\n",
    "#trainer.save_model(local_path)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for '/users/eleves-a/2022/oussama.zouhry/sub-event-detection-in-twitter-streams/challenge_data/fine_tuned_model_bert_cased/checkpoint-12000/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/users/eleves-a/2022/oussama.zouhry/sub-event-detection-in-twitter-streams/challenge_data/fine_tuned_model_bert_cased/checkpoint-12000/' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 2\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m classifier(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/__init__.py:1047\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   1045\u001b[0m             tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1047\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:940\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    937\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 940\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    942\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2016\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2013\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2014\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2019\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '/users/eleves-a/2022/oussama.zouhry/sub-event-detection-in-twitter-streams/challenge_data/fine_tuned_model_bert_cased/checkpoint-12000/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/users/eleves-a/2022/oussama.zouhry/sub-event-detection-in-twitter-streams/challenge_data/fine_tuned_model_bert_cased/checkpoint-12000/' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(task=\"text-classification\", model=local_path)\n",
    "classifier(dataset['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet transformers\n",
    "#import model and Tokenizer\n",
    "from transformers import TFAutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 11:51:44.835398: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733827905.131493  133128 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733827905.214214  133128 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-10 11:51:45.980882: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode: [101, 4372, 16044, 2033, 999, 102]\n",
      "Decode: [CLS] encode me! [SEP]\n"
     ]
    }
   ],
   "source": [
    "#BERT\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "enc = TOKENIZER.encode(\"Encode me!\")\n",
    "dec = TOKENIZER.decode(enc)\n",
    "print(\"Encode: \" + str(enc))\n",
    "print(\"Decode: \" + str(dec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(data,maximum_len) :\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "  \n",
    "    data = data.reset_index(drop=True)\n",
    "    for i in range(len(data[\"Tweet\"])):\n",
    "        encoded = TOKENIZER.encode_plus(data[\"Tweet\"][i],\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=maximum_len,\n",
    "                                        pad_to_max_length=True,\n",
    "                                        return_attention_mask=True)\n",
    "      \n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "        \n",
    "    return np.array(input_ids),np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "\n",
    "def build_model(model_layer, learning_rate, use_meta = USE_META, add_dense = ADD_DENSE,\n",
    "               dense_dim = DENSE_DIM, add_dropout = ADD_DROPOUT, dropout = DROPOUT):\n",
    "    \n",
    "    #define inputs\n",
    "    input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n",
    "    attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n",
    "    meta_input = tf.keras.Input(shape = (train.shape[1], ))\n",
    "    \n",
    "    #insert BERT layer\n",
    "    transformer_layer = model_layer([input_ids,attention_masks])\n",
    "    \n",
    "    #choose only last hidden-state\n",
    "    output = transformer_layer[1]\n",
    "    \n",
    "    #add meta data\n",
    "    if use_meta:\n",
    "        output = tf.keras.layers.Concatenate()([output, meta_input])\n",
    "    \n",
    "    #add dense relu layer\n",
    "    if add_dense:\n",
    "        print(\"Training with additional dense layer...\")\n",
    "        output = tf.keras.layers.Dense(dense_dim,activation='relu')(output)\n",
    "    \n",
    "    #add dropout\n",
    "    if add_dropout:\n",
    "        print(\"Training with dropout...\")\n",
    "        output = tf.keras.layers.Dropout(dropout)(output)\n",
    "    \n",
    "    #add final node for binary classification\n",
    "    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n",
    "    \n",
    "    #assemble and compile\n",
    "    if use_meta:\n",
    "        print(\"Training with meta-data...\")\n",
    "        model = tf.keras.models.Model(inputs = [input_ids,attention_masks, meta_input],outputs = output)\n",
    "    \n",
    "    else:\n",
    "        print(\"Training without meta-data...\")\n",
    "        model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
    "\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
      "0  11_0       11         0          0  1404575400000   \n",
      "1  11_0       11         0          0  1404575400000   \n",
      "2  11_0       11         0          0  1404575400000   \n",
      "3  11_0       11         0          0  1404575400000   \n",
      "4  11_0       11         0          0  1404575400000   \n",
      "\n",
      "                                               Tweet  \n",
      "0  RT @2014WorIdCup: Argentina vs Belgium\\n\\nWho ...  \n",
      "1  @elijahman_ time to focus on Belgium winning t...  \n",
      "2  RT @FIFAWorldCup: GLOBAL STADIUM: #Joinin with...  \n",
      "3  RT @CatholicNewsSvc: #PopeFrancis. Uh-oh. Arge...  \n",
      "4  RT @soccerdotcom: If he scores vs #BEL we'll a...  \n",
      "             ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
      "3805405    1_85        1        85          0  1404854122000   \n",
      "142818    11_58       11        58          1  1404578902000   \n",
      "1001371  10_152       10       152          0  1405286551000   \n",
      "2224280   13_48       13        48          0  1404146327000   \n",
      "2232642   13_52       13        52          0  1404146552000   \n",
      "\n",
      "                                                     Tweet  \n",
      "3805405  RT @josemourinhotv: Suddenly #POR 4-0 loss to ...  \n",
      "142818   RT @Betfair: Lionel Messi responds to claims t...  \n",
      "1001371  “@SocialRMadrid: Injured Di Maria would have d...  \n",
      "2224280  RT @emmanuelojes: My God is bigger than France...  \n",
      "2232642  RT @OptaJoe: 43% - Both sides attacking predom...  \n"
     ]
    }
   ],
   "source": [
    "li = []\n",
    "for filename in os.listdir(\"train_tweets\"):\n",
    "    df = pd.read_csv(\"train_tweets/\" + filename)\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li, ignore_index=True)\n",
    "print(df.head())\n",
    "\n",
    "#split data into training and test set. Train on 70% of the data and test on 30%. Both of them are still pd files\n",
    "train = df.sample(frac=0.7, random_state=42)\n",
    "test = df.drop(train.index)\n",
    "print(train.head())\n",
    "\n",
    "#def scale(df, scaler):\n",
    "#    return scaler.fit_transform(df.iloc[:, 2:])\n",
    "\n",
    "#and scal\n",
    "#meta_train = scale(train, StandardScaler())\n",
    "#meta_test = scale(test, StandardScaler())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733828402.690027  133128 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9031 MB memory:  -> device: 0, name: NVIDIA RTX A2000 12GB, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#get BERT layer\n",
    "bert_large = TFAutoModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "#get BERT tokenizer\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2673: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#get our inputs\n",
    "train_input_ids,train_attention_masks = bert_encode(train,60)\n",
    "test_input_ids,test_attention_masks = bert_encode(test,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 3539235\n",
      "Test length: 1516815\n"
     ]
    }
   ],
   "source": [
    "meta_train = train\n",
    "meta_test = test\n",
    "#debugging step\n",
    "print('Train length:', len(train_input_ids))\n",
    "print('Test length:', len(test_input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training without meta-data...\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 60)]                 0         []                            \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   3351418   ['input_1[0][0]',             \n",
      " )                           ngAndCrossAttentions(last_   88         'input_2[0][0]']             \n",
      "                             hidden_state=(None, 60, 10                                           \n",
      "                             24),                                                                 \n",
      "                              pooler_output=(None, 1024                                           \n",
      "                             ),                                                                   \n",
      "                              past_key_values=None, hid                                           \n",
      "                             den_states=None, attention                                           \n",
      "                             s=None, cross_attentions=N                                           \n",
      "                             one)                                                                 \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1)                    1025      ['tf_bert_model[0][1]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 335142913 (1.25 GB)\n",
      "Trainable params: 335142913 (1.25 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#and build and view parameters\n",
    "BERT_large = build_model(bert_large, learning_rate = 1e-5)\n",
    "BERT_large.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('large_model.h5', monitor='val_loss', save_best_only = True, save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input_ids dtype: int32\n",
      "train_attention_masks dtype: int32\n",
      "meta_train dtype: object\n",
      "train_labels dtype: int32\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta_train dtype:\u001b[39m\u001b[38;5;124m\"\u001b[39m, meta_train\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_labels dtype:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_labels\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m----> 9\u001b[0m history_bert \u001b[38;5;241m=\u001b[39m \u001b[43mBERT_large\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_attention_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEventType\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "#train BERT\n",
    "import numpy as np\n",
    "\n",
    "print(\"train_input_ids dtype:\", train_input_ids.dtype)\n",
    "print(\"train_attention_masks dtype:\", train_attention_masks.dtype)\n",
    "print(\"meta_train dtype:\", meta_train.dtype)\n",
    "print(\"train_labels dtype:\", train_labels.dtype)\n",
    "\n",
    "history_bert = BERT_large.fit([train_input_ids,train_attention_masks, meta_train], train[\"EventType\"],\n",
    "                         validation_split = .2, epochs = EPOCHS, callbacks = [checkpoint], batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model with best losses\n",
    "BERT_large.load_weights('large_model.h5')\n",
    "\n",
    "preds_bert = BERT_large.predict([test_input_ids,test_attention_masks,meta_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as dataframe\n",
    "submission_bert = pd.DataFrame()\n",
    "submission_bert['id'] = test_id\n",
    "submission_bert['prob'] = preds_bert\n",
    "submission_bert['target'] = np.round(submission_bert['prob']).astype(int)\n",
    "submission_bert.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Perform the search on the training set\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters\u001b[39;00m\n\u001b[1;32m     16\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=50, max_iter=5000)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1.0, 10, 20, 30, 50, 100],  # Regularization strengths\n",
    "    'solver': ['lbfgs', 'liblinear', 'saga'],  # Solvers\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV \n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "# Perform the search on the training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters: \", best_params) \n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Test set logistic accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\"\"\" \n",
    "cv = 5\n",
    "Best hyperparameters:  {'C': 10, 'solver': 'lbfgs'}\n",
    "Test set logistic accuracy:  0.7679127725856698 \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "cv = 10\n",
    "Best hyperparameters:  {'C': 30, 'solver': 'lbfgs'}\n",
    "Test set logistic accuracy:  0.7679127725856698\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" Better preprocessing  Best hyperparameters:  {'C': 10, 'solver': 'saga'}\n",
    "Test set logistic accuracy:  0.7725856697819314\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set logistic accuracy:  0.7663551401869159\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=50, max_iter=4000, C=30, solver='lbfgs')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set logistic accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best hyperparameters:  {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Test set SVM accuracy:  0.7570093457943925\n"
     ]
    }
   ],
   "source": [
    "# Define the SVM model\n",
    "model = SVC(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1.0, 10, 100],          # Regularization strengths\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # Kernel types\n",
    "    'gamma': ['scale', 'auto']               # Kernel coefficient (optional for 'rbf', 'poly', 'sigmoid')\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Perform the search on the training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Test set SVM accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\"\"\" Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
    "Best hyperparameters:  {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
    "Test set SVM accuracy:  0.7570093457943925 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set SVM accuracy:  0.7679127725856698\n"
     ]
    }
   ],
   "source": [
    "# Logistic regressor\n",
    "#clf = LogisticRegression(random_state=42, max_iter=1000, C=1.0, solver='lbfgs').fit(X_train, y_train)\n",
    "#y_pred = clf.predict(X_test)\n",
    "#print(\"Test set logistic: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# SVMs\n",
    "clf = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set SVM accuracy: \", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 135 candidates, totalling 675 fits\n",
      "Best hyperparameters:  {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Test set Decision Tree accuracy:  0.7040498442367601\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define the Decision Tree model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # Splitting criterion\n",
    "    'max_depth': [None, 10, 20, 30, 40],           # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],               # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],                 # Minimum samples required to be a leaf node\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Perform the search on the training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Test set Decision Tree accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\"\"\" \n",
    "Fitting 5 folds for each of 135 candidates, totalling 675 fits\n",
    "Best hyperparameters:  {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
    "Test set Decision Tree accuracy:  0.7040498442367601\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecisionTreeClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTreeClassifier\u001b[49m(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      2\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DecisionTreeClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set Decision Tree accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Test set Random Forest accuracy:  0.7772585669781932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nBest hyperparameters:  {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\\nTest set Random Forest accuracy:  0.8052959501557633\\n\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the Random Forest model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50,],             # Number of trees\n",
    "    'max_depth': [None],            # Maximum depth of each tree\n",
    "    'min_samples_split': [2],            # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': [1],              # Minimum samples required to be at a leaf node\n",
    "    'max_features': ['sqrt'],     # Number of features to consider for splitting\n",
    "    'bootstrap': [True],                 # Whether to use bootstrapped samples\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Perform the search on the training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Test set Random Forest accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\"\"\"\n",
    "Best hyperparameters:  {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
    "Test set Random Forest accuracy:  0.8052959501557633\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set Random Forest accuracy:  0.7850467289719626\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set Random Forest accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [23:47:22] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [23:48:20] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [23:49:16] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [23:50:17] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [23:51:08] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [23:51:53] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [23:52:39] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [23:53:37] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [23:54:31] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [23:55:28] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [23:56:22] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [23:57:11] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [23:58:01] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [23:58:49] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [23:59:36] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:00:24] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:02:04] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:03:48] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Perform the search on the training set\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters\u001b[39;00m\n\u001b[1;32m     23\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/model_selection/_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m     )\n\u001b[0;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/xgboost/sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1529\u001b[0m )\n\u001b[0;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define the XGBoost model\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],         # Number of trees in the ensemble\n",
    "    'max_depth': [3, 5, 7, 10],            # Maximum depth of a tree\n",
    "    'learning_rate': [0.01, 0.1, 0.2],     # Step size shrinkage\n",
    "    'subsample': [0.6, 0.8, 1.0],          # Fraction of samples used for training each tree\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],   # Fraction of features used for training each tree\n",
    "    'gamma': [0, 0.1, 0.3, 0.5],           # Minimum loss reduction required to make a split\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Perform the search on the training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Test set XGBoost accuracy: \", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [02:40:38] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set XGBoost accuracy:  0.7881619937694704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/oussama.zouhry/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [02:40:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTest set XGBoost accuracy:  0.7959501557632399\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, device=device)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set XGBoost accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\"\"\"\n",
    "Test set XGBoost accuracy:  0.7959501557632399\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the MLP model\n",
    "model = MLPClassifier(max_iter=2000, random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],  # Number of neurons in each layer\n",
    "    'activation': ['tanh', 'relu'],                              # Activation function\n",
    "    'solver': ['adam', 'sgd'],                                   # Solver for weight optimization\n",
    "    'alpha': [0.0001, 0.001, 0.01],                              # L2 regularization parameter\n",
    "    'learning_rate': ['constant', 'adaptive'],                   # Learning rate schedule\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Perform the search on the training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Test set MLP accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set MLP accuracy:  0.7694704049844237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Test set MLP accuracy:  0.764797507788162 '"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(max_iter=2000, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set MLP accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\"\"\" Test set MLP accuracy:  0.764797507788162 \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lets us evaluate in GermanyGhana32.csv\n",
      "lets us evaluate in GermanySerbia2010.csv\n",
      "lets us evaluate in GreeceIvoryCoast44.csv\n",
      "lets us evaluate in NetherlandsMexico64.csv\n"
     ]
    }
   ],
   "source": [
    "###### For Kaggle submission\n",
    "predictions = []\n",
    "\n",
    "# We read each file separately, we preprocess the tweets and then use the classifier to predict the labels.\n",
    "# Finally, we concatenate all predictions into a list that will eventually be concatenated and exported\n",
    "# to be submitted on Kaggle.\n",
    "for fname in os.listdir(\"eval_tweets_preprocessed\"):\n",
    "    print(\"lets us evaluate in\", fname)\n",
    "    val_df = pd.read_csv(\"eval_tweets_preprocessed/\" + fname)\n",
    "\n",
    "    #tweet_vectors = np.vstack([get_avg_embedding(str(tweet), embeddings_model, 384) for tweet in val_df['Tweet']])\n",
    "    #tweet_vectors = np.vstack([get_sum_embedding(str(tweet), embeddings_model, 384) for tweet in val_df['Tweet']])\n",
    "    tweet_vectors = np.vstack([get_min_embedding(str(tweet), embeddings_model, 200) for tweet in val_df['Tweet']])\n",
    "    \n",
    "    #tweet_vectors = model.encode(\n",
    "    #df['Tweet'].astype(str).tolist(),\n",
    "    #show_progress_bar=True,\n",
    "    #batch_size=32,  # Adjust batch size based on available GPU memory\n",
    "    #convert_to_tensor=True\n",
    "    #)\n",
    "    #tweet_vectors = tweet_vectors.cpu().numpy()  # Move to CPU if necessary\n",
    "\n",
    "    tweet_df = pd.DataFrame(tweet_vectors)\n",
    "\n",
    "    period_features = pd.concat([val_df, tweet_df], axis=1)\n",
    "    period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
    "    period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "    X = period_features.drop(columns=['MatchID', 'PeriodID', 'ID']).values\n",
    "\n",
    "    preds = clf.predict(X)\n",
    "\n",
    "    period_features['EventType'] = preds\n",
    "\n",
    "    predictions.append(period_features[['ID', 'EventType']])\n",
    "\n",
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv('linear_regression_better_preprocessing_min_embedding.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNet + 2 dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CONV1D_model(vectorize_layer, embedding_dim, embedding_matrix, dropout_rate_1, kernels_1, kernel_size_1, strides_1, kernels_2, kernel_size_2, strides_2, units_1, dropout_rate_2, lr):\n",
    "    \n",
    "    text_input = tf.keras.Input(shape=(None,), dtype=tf.string, name='text')\n",
    "    x = vectorize_layer(text_input)\n",
    "    x = layers.Embedding(input_dim = vectorize_layer.vocabulary_size(), \n",
    "                        output_dim = embedding_dim, \n",
    "                        weights = [embedding_matrix], \n",
    "                        trainable=False)(x)\n",
    "    x = layers.Dropout(dropout_rate_1)(x)\n",
    "\n",
    "    # Conv1D + global max pooling\n",
    "    x = layers.Conv1D(kernels_1, kernel_size_1, padding=\"valid\", activation=\"relu\", strides=strides_1)(x)\n",
    "    x = layers.Conv1D(kernels_2, kernel_size_2, padding=\"valid\", activation=\"relu\", strides=strides_2)(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = layers.Dense(units_1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate_2)(x)\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    output = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "    model = tf.keras.Model(text_input, output, name='Simple_1D_ConvNet')\n",
    "    \n",
    "    # Compile the model\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorize_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Build the model\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m get_CONV1D_model(\u001b[43mvectorize_layer\u001b[49m, embedding_dim, embedding_matrix, dropout_rate_1, kernels_1, kernel_size_1, strides_1, kernels_2, kernel_size_2, strides_2, units_1, dropout_rate_2, lr)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorize_layer' is not defined"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters (F1 score = 0.7619208087615839 on 5-fold cross-validation)\n",
    "batch_size = 48\n",
    "embedding_dim = 50\n",
    "dropout_rate_1 = 0.30000000000000004\n",
    "kernels_1 = 96\n",
    "kernel_size_1 = 2\n",
    "strides_1 = 1\n",
    "kernels_2 = 256\n",
    "kernel_size_2 = 2\n",
    "strides_2 = 1\n",
    "units_1 = 256\n",
    "dropout_rate_2 = 0.1\n",
    "lr = 0.0005479929838006311\n",
    "epochs = 15\n",
    "\n",
    "# Build the model\n",
    "model = get_CONV1D_model(vectorize_layer, embedding_dim, embedding_matrix, dropout_rate_1, kernels_1, kernel_size_1, strides_1, kernels_2, kernel_size_2, strides_2, units_1, dropout_rate_2, lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
